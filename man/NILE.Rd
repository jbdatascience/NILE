% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/source.R
\name{NILE}
\alias{NILE}
\title{NILE estimator}
\usage{
NILE(
  Y,
  X,
  A,
  lambda.star = "test",
  intercept = TRUE,
  df = 100,
  x.new = NULL,
  test = "penalized",
  p.min = 0.05,
  plot = TRUE,
  f.true = NULL,
  par.x = list(),
  par.a = list(),
  par.cv = list(num.folds = 10, optim = "optimize")
)
}
\arguments{
\item{Y}{A numeric vector with observations from the target variable}

\item{X}{A numeric vector with observations from the predictor variable}

\item{A}{A numeric vector with observations from the exogenous variable}

\item{lambda.star}{either a positive numeric, \code{Inf} or \code{"test"};
weight which determines the relative importance of the OLS loss and the TSLS
loss, see details.}

\item{intercept}{logical; indicating whether an intercept should be included into
the regression model}

\item{df}{positive integer; number of basis splines used
to model the nonlinear function \eqn{X\to Y}{X -> Y}}

\item{x.new}{numeric; x-values at which predictions are desried}

\item{test}{character; !!!}

\item{p.min}{numeric between 0 and 1; the significance level at
which the test which determines lambda.star should be tested.}

\item{plot}{logical; diagnostic plots}

\item{f.true}{real-valued function of one variable; If the groundtruth is known,
it can be suplied and will be included in diagnostic plots.}

\item{par.x}{a list of different parameters determining the \eqn{B}-spline regression
of \eqn{Y} onto \eqn{X}
\itemize{
\item \code{breaks} numeric vector; knots at which B-spline is placed
\item \code{num.breaks} positive interger; number of knots used (ignored if breaks is supplied)
\item \code{n.order} positive integer; order of B-spline basis (default is 4, which corresponds
to cubic splines)
\item \code{pen.degree} positive integer; 1 corresponds to a penalty on the first,
and 2 (default) corresponds to a penalty on the second order derivative.
}}

\item{par.a}{a list of different parameters determining the B-spline regression
of the residuals \eqn{Y - B(X)\beta} onto the residuals \eqn{A- ???} !!!
\itemize{
\item \code{breaks} same as above
\item \code{num.breaks} same as above
\item \code{n.order} same as above
\item \code{pen.degree} same as above
}}

\item{par.cv}{A list with parameters for the cross-validation procedure.
\itemize{
\item \code{num.folds} either "leave-one-out" or positive integer; number of CV folds (default = 10)
\item \code{optim} one of "optimize" (default) or "grid.search"; optimization method used for cross-validation.
\item \code{n.grid} positive integer; number of grid points used in grid search
}}
}
\value{
An object of class AR, containing the following elements
\itemize{
\item \code{coefficients} estimated splines coefficients for the relationship \eqn{X\to Y}{X -> Y}
\item \code{residuals} prediction residuals
\item \code{fitted.values} fitted values
\item \code{betaA} estimated spline coefficients for the regression of the prediction
residuals onto the variable \eqn{A}
\item \code{fitA} fitted values for the regression of the prediction
residuals onto the variable \eqn{A}
\item \code{Y} the response variable
\item \code{BX} basis object holding all relevant information about the B-spline
basis used for the regression \eqn{X\to Y}{X -> Y}
\item \code{BA} basis object holding all relevant information about the B-spline
basis used for the regression of the residuals onto A
\item \code{ols} OLS loss at the estimated parameters
\item \code{iv} TSLS loss at the estimated parameters
\item \code{ar} NILE loss at the estimated parameters, that is,
\eqn{OLS(\theta) + \texttt{lambda.star}\times TSLS(\theta)}{OLS +
lambda.star * TSLS}
\item \code{intercept} was an intercept supplied?
\item \code{lambdaX} penalty parameter used for the spline regression
\eqn{X\to Y}{X -> Y}
\item \code{lambdaA} penalty parameter used for the regression of residuals
onto \eqn{A}
\item \code{lambda.star} the (estimated) value for lambda.star
\item \code{pred} predicted values at the supplied vector x.new
}
}
\description{
Method for estimating a nonlinear causal relationship \eqn{X\to Y}{X -> Y}
that is assumed to linearly extrapolate outside of the support
of \eqn{X}.
}
\details{
The NILE estimator can be used to learn a nonlinear causal influence
of a real-valued predictor \eqn{X} on a real-valued response variable \eqn{Y}.
It exploits an instrumental variable setting; it assumes that the
variable \eqn{A} is a valid instrument.
The estimator uses B-splines to estimate the nonlinear relationship. It further
assumes that the causal function extrapolates lienarly outside of the empirical support
of \eqn{X}, and can therefore be used to obtain causal predictions (that is, predicted
values for \eqn{Y} under confounding-removing interventions on \eqn{X}) even for values of
\eqn{X} which lie outside the training support.

On a more technical side, the NILE estimator proceeds as follows. First, two B-splines
\eqn{B = (B_1,...,B_{df})}{B = (B_1,...,B_df)} and \eqn{C = (C_1, ..., C_{df})}{C = (C_1, ..., C_df)}
are constructed, which span the values of \eqn{X} and \eqn{A}, respectively.
These give rise to the loss functions OLS(\eqn{\theta}), which
corresponds to the MSE for the prediction residuals \eqn{Y - \theta^T B}, and
TSLS(\eqn{\theta}),
which are the fitted values of the spline-regression of the residuals
\eqn{Y - \theta^T B}
onto the spline basis \eqn{C}. The NILE estimator the estimates \eqn{\theta}
by minimizing the objective function
\deqn{OLS(\theta) + \texttt{lambda.star}\times TSLS(\theta) + PEN(\theta),}{
OLS(\theta) + lambda.star * TSLS(\theta) + PEN(\theta),}
where PEN(\eqn{\theta}) is a quadratic penalty term which enforces smoothness.

The parameter \code{lambda.star} is chosen as the largest positive value for which the
corresponding solution yields prediction residuals which pass a test for vanishing
product moment with all basis functions \eqn{C_1(A), \dots C_{df}(A)}{C_1(A), ... C_df(A)}.
}
\examples{
   n.splines.true <- 4
   fX <- function(x, extrap, beta){
     bx <- splines::ns(x, knots = seq(from=extrap[1], to=extrap[2],
                                      length.out=(n.splines.true+1))[
                                        -c(1,n.splines.true+1)],
                       Boundary.knots = extrap)
     bx\%*\%beta
   }

   # data generating model
   n <- 200
   set.seed(2)
   beta0 <- runif(n.splines.true, -1,1)
   alphaA <- alphaEps <- alphaH <- 1/sqrt(3)
   A <- runif(n,-1,1)
   H <- runif(n,-1,1)
   X <- alphaA * A + alphaH * H + alphaEps*runif(n,-1,1)
   Y <- fX(x=X,extrap=c(-.7,.7), beta=beta0) + .3 * H + .2 * runif(n,-1,1)

   x.new <- seq(-2,2,length.out=100)
   f.new <- fX(x=x.new,extrap=c(-.7,.7), beta=beta0)
   plot(X,Y, pch=20)
   lines(x.new,f.new,col="#0072B2",lwd=3)

   ## FIT!
   fit <- NILE(Y, # response
               X, # predictors (so far, only 1-dim supported)
               A, # anchors (1 or 2-dim, although 2-dim is experimental so far)
               lambda.star = "test", # (0 = OLS, Inf = IV, (0,Inf) =
               # nonlinear anchor regression, "test" = NILE)
               test = "penalized",
               intercept = TRUE,
               df = 50, # number of splines used for X -> Y
               p.min = 0.05, # level at which test for lambda is performed
               x.new = x.new, # values at which predictions are required
               plot=TRUE, # diagnostics plots
               f.true = function(x) fX(x,c(-.7,.7), beta0), # if supplied, the
               # true causal function is added to the plot
               par.x = list(lambda=NULL, # positive smoothness penalty for X -> Y,
                            # if NULL, it is chosen by CV to minimize out-of-sample
                            # AR objective
                            breaks=NULL, # if breaks are supplied, exactly these
                            # will be used for splines basis
                            num.breaks=20, # will result in num.breaks+2 splines,
                            # ignored if breaks is supplied.
                            n.order=4 # order of splines
               ),
               par.a = list(lambda=NULL, # positive smoothness penalty for fit of
                            # residuals onto A. If NULL, we first compute the OLS
                            # fit of Y onto X,
                            # and then choose lambdaA by CV to
                            # minimize the out-of-sample MSE for predicting
                            # the OLS residuals
                            breaks=NULL, # same as above
                            num.breaks=4, # same as above
                            n.order=4 # same as above
               ))

}
\references{
\insertRef{rune2020}{NILE}
}
\author{
Rune Christiansen \email{krunechristiansen@math.ku.dk}
}
